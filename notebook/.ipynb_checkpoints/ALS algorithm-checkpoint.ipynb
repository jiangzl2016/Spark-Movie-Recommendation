{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows building a recommendation system using ALS algorithm on MovieLens Dataset. The algorithm is demonstrated in Harper and Konstan's 2015 paper. The notebook implements ALS algorithm through both its primitive way and the use of MLlib way. Finally, the algorithm returns a list of recommended movies to input users. \n",
    "\n",
    "Reference: \n",
    "1. (Spark ML library) https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html\n",
    "2. F. Maxwell Harper, Joseph A. Konstan. 2015. The Movie Lens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems, December 2015. DOI = https://dx.doi.org/10.1145/2827872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "findspark.init(\"../../spark-2.2.0-bin-hadoop2.7\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Column, Row, functions as F \n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"ALS algorithm\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movies = spark.read.load(\"../data/ml-latest-small/movies.csv\", format='csv', header = True)\n",
    "ratings = spark.read.load(\"../data/ml-latest-small/ratings.csv\", format='csv', header = True)\n",
    "links = spark.read.load(\"../data/ml-latest-small/links.csv\",format='csv', header = True)\n",
    "tags = spark.read.load(\"../data/ml-latest-small/tags.csv\",format='csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating| timestamp|\n",
      "+------+-------+------+----------+\n",
      "|     1|     31|   2.5|1260759144|\n",
      "|     1|   1029|   3.0|1260759179|\n",
      "|     1|   1061|   3.0|1260759182|\n",
      "|     1|   1129|   2.0|1260759185|\n",
      "|     1|   1172|   4.0|1260759205|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert types\n",
    "ratings = ratings.select(ratings.userId.cast(\"integer\"), \n",
    "                        ratings.movieId.cast(\"integer\"),\n",
    "                        ratings.rating.cast(\"float\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build ALS Model Using MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.mllib.recommendation import MatrixFactorizationModel, Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_data, test_data) = ratings.randomSplit([0.8,0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: float]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.cache()\n",
    "test_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "# Set Colde Start Stratgy false in case we run into users we have no data before\n",
    "ALS_model = ALS(userCol = \"userId\", itemCol = \"movieId\", ratingCol = \"rating\", seed= 245, nonnegative= True, \n",
    "                coldStartStrategy=\"drop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We define grid search algorithm similar to GridSearchCV \n",
    "# using ParamBuilder\n",
    "model = ALS_model\n",
    "search_grid = ParamGridBuilder().addGrid(model.rank, [9,10,11,12])\\\n",
    "                .addGrid(model.maxIter, [17])\\\n",
    "                .addGrid(model.regParam, [0.18, 0.2, 0.22])\\\n",
    "                .build()\n",
    "# We define loss function to be root mean square error\n",
    "loss_eval = RegressionEvaluator(metricName = \"rmse\", labelCol = \"rating\", predictionCol = \"prediction\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define Cross Validator\n",
    "# The default will use 3 folds cross_validation \n",
    "cv_estimator = CrossValidator(\n",
    "                estimator = model,\n",
    "                estimatorParamMaps = search_grid,\n",
    "                evaluator = loss_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit models. Return a set of models 4 x 3 x 3 = 36 models\n",
    "cv_model = cv_estimator.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model = cv_model.bestModel\n",
    "predictions  = best_model.transform(test_data)\n",
    "error = loss_eval.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean square error is:  0.90020456948439\n",
      "The best model has rank:  9 . MaxIter:  17\n",
      "The model has regularization parameter:  0.18\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean square error is: \", error)\n",
    "print(\"The best model has rank: \", best_model.rank, \". MaxIter: \", best_model._java_obj.parent().getMaxIter())\n",
    "print(\"The model has regularization parameter: \", best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model \n",
    "import os, tempfile\n",
    "#path = tempfile.mkdtemp()\n",
    "best_model.save(\"../model/best_model\")\n",
    "#nnn = ALS.load(\"../model/best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+\n",
      "|userId|movieId|rating|prediction|\n",
      "+------+-------+------+----------+\n",
      "|     1|   2294|   2.0| 2.1083632|\n",
      "|     1|   1129|   2.0| 2.0693965|\n",
      "|     1|   1029|   3.0| 2.3426754|\n",
      "|     1|   1061|   3.0| 2.3465362|\n",
      "|     1|   2105|   4.0| 1.9532092|\n",
      "+------+-------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# actual rating v.s predicted rating in test set for user 1\n",
    "predictions.sort(\"userId\", \"rating\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate top N movies for each user based on predicted rating\n",
    "N = 5\n",
    "user_recs = best_model.recommendForAllUsers(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|     recommendations|\n",
      "+------+--------------------+\n",
      "|   471|[[83318,5.1466894...|\n",
      "|   463|[[83318,5.098796]...|\n",
      "|   496|[[59684,5.426467]...|\n",
      "|   148|[[83318,5.8606596...|\n",
      "|   540|[[73290,5.7574763...|\n",
      "|   392|[[3216,4.855725],...|\n",
      "|   243|[[83318,5.0760756...|\n",
      "|   623|[[83318,5.6917357...|\n",
      "|    31|[[83318,5.6117496...|\n",
      "|   516|[[83318,5.0901046...|\n",
      "+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_recs.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "The column 'recommendations' is in a special format where it wraps up movieId and rating etc. information. Therefore, it would be more readable if we properly expand this column.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(df):\n",
    "    movieId = df.select(\"userId\",\"recommendations.movieId\").toPandas().iloc[0,0]\n",
    "    ratings = df.select(\"userId\",\"recommendations.rating\").toPandas().iloc[0,0]\n",
    "    new_df = pd.DataFrame(movieId, columns=[\"movieId\"])\n",
    "    new_df['rating'] = ratings\n",
    "    new_sp_df = sqlContext.createDataFrame(new_df)\n",
    "    return new_sp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_user_recs = convert(user_recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|movieId|            rating|\n",
      "+-------+------------------+\n",
      "|  83318| 5.146689414978027|\n",
      "|  67504| 5.146689414978027|\n",
      "|  83411| 5.146689414978027|\n",
      "|  83359| 5.146689414978027|\n",
      "|   4755|4.8563127517700195|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_user_recs.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
